{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0221d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76068c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'E:\\Projetos\\llm_security\\dataset'\n",
    "path_model = r'E:\\Projetos\\llm_security\\models'\n",
    "\n",
    "train_df = pd.read_parquet(f'{path}/train_net.parquet')\n",
    "test_df = pd.read_parquet(f'{path}/test_net.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pr√©-processador salvo em 'preprocessor_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Selecting only the necessary features\n",
    "features_to_use = ['protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes']\n",
    "categorical_features = ['protocol_type', 'service', 'flag']\n",
    "numerical_features = ['src_bytes', 'dst_bytes']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # This drops the columns that are not explicitly transformed\n",
    ")\n",
    "\n",
    "# Apply preprocessing to both training and testing data\n",
    "X_train = preprocessor.fit_transform(train_df[features_to_use])\n",
    "X_test = preprocessor.transform(test_df[features_to_use])\n",
    "\n",
    "# Convert to CSR format for models that require it\n",
    "X_train_csr = csr_matrix(X_train)\n",
    "X_test_csr = csr_matrix(X_test)\n",
    "\n",
    "# Convert to dense format for deep learning models\n",
    "X_train_dense = X_train_csr.toarray()\n",
    "X_test_dense = X_test_csr.toarray()\n",
    "\n",
    "y_train = train_df['binary_label']\n",
    "y_test = test_df['binary_label']\n",
    "\n",
    "# Salvar o pipeline de pr√©-processamento\n",
    "joblib.dump(preprocessor, f'{path_model}/preprocessor_model.joblib')\n",
    "print(\"‚úÖ Pr√©-processador salvo em 'preprocessor_model.joblib'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9203ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Treinando: Random Forest\n",
      "‚è±Ô∏è Tempo de treinamento: 5.93 segundos\n",
      "‚úÖ Acur√°cia: 0.8225\n",
      "üéØ ROC AUC: 0.9249\n",
      "‚≠ê F1-score: 0.8312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.89      0.81      9711\n",
      "           1       0.91      0.77      0.83     12833\n",
      "\n",
      "    accuracy                           0.82     22544\n",
      "   macro avg       0.83      0.83      0.82     22544\n",
      "weighted avg       0.84      0.82      0.82     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.002470 s\n",
      "\n",
      "üîç Treinando: KNN\n",
      "‚è±Ô∏è Tempo de treinamento: 0.01 segundos\n",
      "‚úÖ Acur√°cia: 0.8244\n",
      "üéØ ROC AUC: 0.8902\n",
      "‚≠ê F1-score: 0.8315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.91      0.82      9711\n",
      "           1       0.92      0.76      0.83     12833\n",
      "\n",
      "    accuracy                           0.82     22544\n",
      "   macro avg       0.83      0.83      0.82     22544\n",
      "weighted avg       0.84      0.82      0.83     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.126293 s\n",
      "\n",
      "üîç Treinando: Decision Tree\n",
      "‚è±Ô∏è Tempo de treinamento: 0.12 segundos\n",
      "‚úÖ Acur√°cia: 0.8194\n",
      "üéØ ROC AUC: 0.9165\n",
      "‚≠ê F1-score: 0.8289\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.89      0.81      9711\n",
      "           1       0.90      0.77      0.83     12833\n",
      "\n",
      "    accuracy                           0.82     22544\n",
      "   macro avg       0.82      0.83      0.82     22544\n",
      "weighted avg       0.83      0.82      0.82     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.000074 s\n",
      "\n",
      "üîç Treinando: CatBoost\n",
      "‚è±Ô∏è Tempo de treinamento: 11.16 segundos\n",
      "‚úÖ Acur√°cia: 0.8224\n",
      "üéØ ROC AUC: 0.9588\n",
      "‚≠ê F1-score: 0.8210\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.96      0.82      9711\n",
      "           1       0.96      0.72      0.82     12833\n",
      "\n",
      "    accuracy                           0.82     22544\n",
      "   macro avg       0.84      0.84      0.82     22544\n",
      "weighted avg       0.86      0.82      0.82     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.000632 s\n",
      "\n",
      "üîç Treinando: Gradient Boosting\n",
      "‚è±Ô∏è Tempo de treinamento: 4.25 segundos\n",
      "‚úÖ Acur√°cia: 0.8622\n",
      "üéØ ROC AUC: 0.9326\n",
      "‚≠ê F1-score: 0.8752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.85      9711\n",
      "           1       0.90      0.85      0.88     12833\n",
      "\n",
      "    accuracy                           0.86     22544\n",
      "   macro avg       0.86      0.86      0.86     22544\n",
      "weighted avg       0.87      0.86      0.86     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.000174 s\n",
      "\n",
      "üîç Treinando: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nielsen.castelo\\AppData\\Local\\anaconda3\\envs\\inatel\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [10:30:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Tempo de treinamento: 0.29 segundos\n",
      "‚úÖ Acur√°cia: 0.8138\n",
      "üéØ ROC AUC: 0.9613\n",
      "‚≠ê F1-score: 0.8108\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.96      0.82      9711\n",
      "           1       0.96      0.70      0.81     12833\n",
      "\n",
      "    accuracy                           0.81     22544\n",
      "   macro avg       0.84      0.83      0.81     22544\n",
      "weighted avg       0.85      0.81      0.81     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.000491 s\n",
      "\n",
      "üîç Treinando: Ridge\n",
      "‚è±Ô∏è Tempo de treinamento: 0.14 segundos\n",
      "‚úÖ Acur√°cia: 0.7816\n",
      "üéØ ROC AUC: nan\n",
      "‚≠ê F1-score: 0.7850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.89      0.78      9711\n",
      "           1       0.89      0.70      0.78     12833\n",
      "\n",
      "    accuracy                           0.78     22544\n",
      "   macro avg       0.79      0.79      0.78     22544\n",
      "weighted avg       0.81      0.78      0.78     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.000062 s\n",
      "\n",
      "üîç Treinando: Logistic Regression\n",
      "‚è±Ô∏è Tempo de treinamento: 0.18 segundos\n",
      "‚úÖ Acur√°cia: 0.7816\n",
      "üéØ ROC AUC: 0.9040\n",
      "‚≠ê F1-score: 0.7850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.89      0.78      9711\n",
      "           1       0.89      0.70      0.78     12833\n",
      "\n",
      "    accuracy                           0.78     22544\n",
      "   macro avg       0.79      0.79      0.78     22544\n",
      "weighted avg       0.81      0.78      0.78     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.000049 s\n",
      "\n",
      "üîç Treinando: Perceptron\n",
      "‚è±Ô∏è Tempo de treinamento: 0.04 segundos\n",
      "‚úÖ Acur√°cia: 0.6583\n",
      "üéØ ROC AUC: nan\n",
      "‚≠ê F1-score: 0.5753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.99      0.71      9711\n",
      "           1       0.98      0.41      0.58     12833\n",
      "\n",
      "    accuracy                           0.66     22544\n",
      "   macro avg       0.77      0.70      0.64     22544\n",
      "weighted avg       0.80      0.66      0.64     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.000050 s\n",
      "\n",
      "üîç Treinando: SGD-Lasso\n",
      "‚è±Ô∏è Tempo de treinamento: 0.08 segundos\n",
      "‚úÖ Acur√°cia: 0.7816\n",
      "üéØ ROC AUC: nan\n",
      "‚≠ê F1-score: 0.7850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.89      0.78      9711\n",
      "           1       0.89      0.70      0.78     12833\n",
      "\n",
      "    accuracy                           0.78     22544\n",
      "   macro avg       0.79      0.79      0.78     22544\n",
      "weighted avg       0.81      0.78      0.78     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.000055 s\n",
      "\n",
      "üîç Treinando: SGD-ElasticNet\n",
      "‚è±Ô∏è Tempo de treinamento: 0.08 segundos\n",
      "‚úÖ Acur√°cia: 0.7816\n",
      "üéØ ROC AUC: nan\n",
      "‚≠ê F1-score: 0.7850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.89      0.78      9711\n",
      "           1       0.89      0.70      0.78     12833\n",
      "\n",
      "    accuracy                           0.78     22544\n",
      "   macro avg       0.79      0.79      0.78     22544\n",
      "weighted avg       0.81      0.78      0.78     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.000051 s\n",
      "\n",
      "üîç Treinando: LightGBM\n",
      "[LightGBM] [Info] Number of positive: 58630, number of negative: 67343\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 638\n",
      "[LightGBM] [Info] Number of data points in the train set: 125973, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.465417 -> initscore=-0.138552\n",
      "[LightGBM] [Info] Start training from score -0.138552\n",
      "‚è±Ô∏è Tempo de treinamento: 0.24 segundos\n",
      "‚úÖ Acur√°cia: 0.8405\n",
      "üéØ ROC AUC: 0.9633\n",
      "‚≠ê F1-score: 0.8424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.96      0.84      9711\n",
      "           1       0.96      0.75      0.84     12833\n",
      "\n",
      "    accuracy                           0.84     22544\n",
      "   macro avg       0.85      0.86      0.84     22544\n",
      "weighted avg       0.87      0.84      0.84     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.000809 s\n",
      "\n",
      "üìà Benchmark salvo em 'benchmark_resultados.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, f1_score\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "path_results = r'E:\\Projetos\\llm_security\\results'\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"Ridge\": RidgeClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Perceptron\": Perceptron(),\n",
    "    \"SGD-Lasso\": SGDClassifier(penalty='l1', random_state=42, max_iter=1000),\n",
    "    \"SGD-ElasticNet\": SGDClassifier(penalty='elasticnet', random_state=42, max_iter=1000),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Lista para resultados\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nüîç Treinando: {model_name}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao treinar {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            if len(np.unique(y_test)) == 2:\n",
    "                y_score = model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                y_score = None\n",
    "        else:\n",
    "            y_score = None\n",
    "    except NotFittedError:\n",
    "        print(f\"‚ùå Modelo {model_name} n√£o foi treinado corretamente.\")\n",
    "        continue\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_test, y_score) if y_score is not None else np.nan\n",
    "    except Exception:\n",
    "        roc_auc = np.nan\n",
    "\n",
    "    # F1-score (bin√°rio ou macro para multi-classe)\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "    else:\n",
    "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    print(f\"‚è±Ô∏è Tempo de treinamento: {training_time:.2f} segundos\")\n",
    "    print(f\"‚úÖ Acur√°cia: {acc:.4f}\")\n",
    "    print(f\"üéØ ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"‚≠ê F1-score: {f1:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    num_batches = X_test.shape[0] // 30\n",
    "    inference_times = []\n",
    "    for i in range(num_batches):\n",
    "        batch_X = X_test[i*30:(i+1)*30]\n",
    "        start_inference = time.time()\n",
    "        model.predict(batch_X)\n",
    "        end_inference = time.time()\n",
    "        inference_times.append(end_inference - start_inference)\n",
    "\n",
    "    avg_inference_time = np.mean(inference_times) if inference_times else np.nan\n",
    "    print(f\"‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: {avg_inference_time:.6f} s\")\n",
    "\n",
    "    results.append({\n",
    "        \"Modelo\": model_name,\n",
    "        \"Acur√°cia\": acc,\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"F1-score\": f1,\n",
    "        \"Tempo Treinamento (s)\": training_time,\n",
    "        \"Tempo Infer√™ncia M√©dio (s)\": avg_inference_time\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_excel(f'{path_results}/benchmark_resultados.xlsx', index=False)\n",
    "print(\"\\nüìà Benchmark salvo em 'benchmark_resultados.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf85d69",
   "metadata": {},
   "source": [
    "\n",
    "### **1. O que s√£o os dados?**\n",
    "\n",
    "**KDD Cup 99**, que representa:\n",
    "\n",
    "* **Conex√µes de rede** entre computadores (ex: ao acessar um site, enviar um e-mail, etc.)\n",
    "* Cada conex√£o √© descrita por **v√°rias caracter√≠sticas** (ou \"atributos\").\n",
    "\n",
    "Exemplos de atributos:\n",
    "\n",
    "| Nome            | O que representa                        |\n",
    "| --------------- | --------------------------------------- |\n",
    "| `protocol_type` | Tipo de protocolo (TCP, UDP, ICMP)      |\n",
    "| `service`       | Servi√ßo acessado (HTTP, FTP, SSH, etc.) |\n",
    "| `flag`          | Estado da conex√£o                       |\n",
    "| `src_bytes`     | Quantidade de dados enviados            |\n",
    "| `dst_bytes`     | Quantidade de dados recebidos           |\n",
    "\n",
    "---\n",
    "\n",
    "###  **2. O que √© o r√≥tulo (label)?**\n",
    "\n",
    "Cada linha dos dados possui uma **etiqueta (chamada `label`)**, indicando:\n",
    "\n",
    "* **\"normal\"** ‚Üí se a conex√£o √© leg√≠tima\n",
    "* Ou o **tipo de ataque** (ex: DoS, Probe, etc.)\n",
    "\n",
    "Neste c√≥digo, o label √© convertido para **apenas dois valores**:\n",
    "\n",
    "* `0` = conex√£o normal\n",
    "* `1` = algum tipo de ataque\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Al√©m disso, voc√™ mede o tempo de:\n",
    "\n",
    "*  **Treinamento** do modelo\n",
    "*  **Infer√™ncia (previs√£o)** sobre pacotes (em grupos de 30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194865b9",
   "metadata": {},
   "source": [
    "# Treinar o melhor modelo o Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Treinando: Gradient Boosting\n",
      "‚è±Ô∏è Tempo de treinamento: 6.09 segundos\n",
      "‚úÖ Acur√°cia: 0.8622\n",
      "üéØ ROC AUC: 0.9326\n",
      "‚≠ê F1-score: 0.8752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.85      9711\n",
      "           1       0.90      0.85      0.88     12833\n",
      "\n",
      "    accuracy                           0.86     22544\n",
      "   macro avg       0.86      0.86      0.86     22544\n",
      "weighted avg       0.87      0.86      0.86     22544\n",
      "\n",
      "‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: 0.000155 s\n",
      "\n",
      "üìà Resultado salvo em 'resultados_gradient_boosting.xlsx'\n",
      "‚úÖ Modelo Gradient Boosting salvo como 'gradient_boosting_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, f1_score\n",
    "import joblib\n",
    "\n",
    "model_name = \"Gradient Boosting\"\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "print(f\"\\nüîç Treinando: {model_name}\")\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Previs√£o\n",
    "y_pred = model.predict(X_test)\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    y_score = model.predict_proba(X_test)[:, 1] if len(np.unique(y_test)) == 2 else None\n",
    "else:\n",
    "    y_score = None\n",
    "\n",
    "# M√©tricas\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_test, y_score) if y_score is not None else np.nan\n",
    "except Exception:\n",
    "    roc_auc = np.nan\n",
    "\n",
    "f1 = f1_score(y_test, y_pred) if len(np.unique(y_test)) == 2 else f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(f\"‚è±Ô∏è Tempo de treinamento: {training_time:.2f} segundos\")\n",
    "print(f\"‚úÖ Acur√°cia: {acc:.4f}\")\n",
    "print(f\"üéØ ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"‚≠ê F1-score: {f1:.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Tempo m√©dio de infer√™ncia para lotes de 30 amostras\n",
    "num_batches = X_test.shape[0] // 30\n",
    "inference_times = []\n",
    "for i in range(num_batches):\n",
    "    batch_X = X_test[i*30:(i+1)*30]\n",
    "    start_inference = time.time()\n",
    "    model.predict(batch_X)\n",
    "    end_inference = time.time()\n",
    "    inference_times.append(end_inference - start_inference)\n",
    "\n",
    "avg_inference_time = np.mean(inference_times) if inference_times else np.nan\n",
    "print(f\"‚öôÔ∏è Tempo m√©dio de infer√™ncia por 30 amostras: {avg_inference_time:.6f} s\")\n",
    "\n",
    "# Salvar resultados em Excel\n",
    "df_results = pd.DataFrame([{\n",
    "    \"Modelo\": model_name,\n",
    "    \"Acur√°cia\": acc,\n",
    "    \"ROC AUC\": roc_auc,\n",
    "    \"F1-score\": f1,\n",
    "    \"Tempo Treinamento (s)\": training_time,\n",
    "    \"Tempo Infer√™ncia M√©dio (s)\": avg_inference_time\n",
    "}])\n",
    "df_results.to_excel(f'{path_results}/resultados_gradient_boosting.xlsx', index=False)\n",
    "print(\"\\nüìà Resultado salvo em 'resultados_gradient_boosting.xlsx'\")\n",
    "\n",
    "# Salvar modelo treinado\n",
    "joblib.dump(model, f'{path_model}/gradient_boosting_model.joblib')\n",
    "print(\"‚úÖ Modelo Gradient Boosting salvo como 'gradient_boosting_model.joblib'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_inatel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
